2020-11-01 19:49:57,490 maskrcnn_benchmark INFO: Using 1 GPUs
2020-11-01 19:49:57,490 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_R_101_FPN_1x.yaml', distributed=False, local_rank=0, opts=['SOLVER.IMS_PER_BATCH', '4', 'TEST.IMS_PER_BATCH', '1', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '90000', 'SOLVER.STEPS', '(45000, 60000)', 'GLOVE_DIR', 'checkpoint/share/glove.6B', 'MODEL.PRETRAINED_DETECTOR_CKPT', 'checkpoint/pretrain_faster_rcnn/model_final.pth', 'OUTPUT_DIR', 'data/relation_motif', 'SOLVER.PRE_VAL', 'False'], skip_test=False)
2020-11-01 19:49:57,490 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2020-11-01 19:49:59,346 maskrcnn_benchmark INFO: 
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.5 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: GeForce GTX 1080 Ti
Nvidia driver version: 455.32.00
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl  
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py36he904b0f_0  
[conda] mkl_fft                   1.2.0            py36h23d657b_0  
[conda] mkl_random                1.1.1            py36h0573a6f_0  
[conda] pytorch                   1.2.0           py3.6_cuda10.0.130_cudnn7.6.2_0    pytorch
[conda] torchvision               0.4.0                py36_cu100    pytorch
        Pillow (8.0.1)
2020-11-01 19:49:59,346 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_R_101_FPN_1x.yaml
2020-11-01 19:49:59,347 maskrcnn_benchmark INFO: 
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/MSRA/R-101"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 512
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 64
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 2048
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 64
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: True
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #PREDICTOR: "MotifPredictor"
    PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "CausalAnalysisPredictor"
    #PREDICTOR: "TransformerPredictor"
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    # Parameter for Transformer Module
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048 
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
OUTPUT_DIR: './output/relation_baseline'
TEST:
  RELATION:
    REQUIRE_OVERLAP: True
    LATER_NMS_PREDICTION_THRES: 0.5

2020-11-01 19:49:59,349 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GLOVE_DIR: checkpoint/share/glove.6B
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (800,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: checkpoint/pretrain_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 1
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 64
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 2048
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 64
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: False
    BATCH_SIZE_PER_IMAGE: 64
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: False
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: False
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    LABEL_SMOOTHING_LOSS: True
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: True
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 512
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/MSRA/R-101
OUTPUT_DIR: data/relation_motif
PATHS_CATALOG: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.01
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 4
  MAX_ITER: 90000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (45000, 60000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 1
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: True
    SYNC_GATHER: False
  SAVE_PROPOSALS: False
2020-11-01 19:49:59,349 maskrcnn_benchmark INFO: Saving config into: data/relation_motif/config.yml
2020-11-01 19:49:59,383 maskrcnn_benchmark INFO: #################### prepare training ####################
2020-11-01 19:50:01,475 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-01 19:50:01,475 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2020-11-01 19:50:24,011 maskrcnn_benchmark.data.build INFO: finish
2020-11-01 19:50:24,011 maskrcnn_benchmark.data.build INFO: Save data statistics to: data/relation_motif/VG_stanford_filtered_with_attribute_train_statistics.cache
2020-11-01 19:50:24,011 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-01 19:58:33,284 maskrcnn_benchmark INFO: #################### end model construction ####################
2020-11-01 19:58:33,549 maskrcnn_benchmark INFO: #################### end optimizer and shcedule ####################
2020-11-01 19:58:33,553 maskrcnn_benchmark INFO: #################### end distributed ####################
2020-11-01 19:58:33,554 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from checkpoint/pretrain_faster_rcnn/model_final.pth
2020-11-01 19:59:11,638 maskrcnn_benchmark INFO: Using 1 GPUs
2020-11-01 19:59:11,639 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_R_101_FPN_1x.yaml', distributed=False, local_rank=0, opts=['SOLVER.IMS_PER_BATCH', '4', 'TEST.IMS_PER_BATCH', '1', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '90000', 'SOLVER.STEPS', '(45000, 60000)', 'GLOVE_DIR', 'checkpoint/share/glove.6B', 'MODEL.PRETRAINED_DETECTOR_CKPT', 'checkpoint/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', 'data/relation_motif', 'SOLVER.PRE_VAL', 'False'], skip_test=False)
2020-11-01 19:59:11,639 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2020-11-01 19:59:13,064 maskrcnn_benchmark INFO: 
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.5 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: GeForce GTX 1080 Ti
Nvidia driver version: 455.32.00
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl  
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py36he904b0f_0  
[conda] mkl_fft                   1.2.0            py36h23d657b_0  
[conda] mkl_random                1.1.1            py36h0573a6f_0  
[conda] pytorch                   1.2.0           py3.6_cuda10.0.130_cudnn7.6.2_0    pytorch
[conda] torchvision               0.4.0                py36_cu100    pytorch
        Pillow (8.0.1)
2020-11-01 19:59:13,064 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_R_101_FPN_1x.yaml
2020-11-01 19:59:13,065 maskrcnn_benchmark INFO: 
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/MSRA/R-101"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 512
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 64
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 2048
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 64
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: True
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #PREDICTOR: "MotifPredictor"
    PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "CausalAnalysisPredictor"
    #PREDICTOR: "TransformerPredictor"
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    # Parameter for Transformer Module
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048 
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
OUTPUT_DIR: './output/relation_baseline'
TEST:
  RELATION:
    REQUIRE_OVERLAP: True
    LATER_NMS_PREDICTION_THRES: 0.5

2020-11-01 19:59:13,067 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GLOVE_DIR: checkpoint/share/glove.6B
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (800,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: checkpoint/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 1
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 64
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 2048
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 64
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: False
    BATCH_SIZE_PER_IMAGE: 64
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: False
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: False
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    LABEL_SMOOTHING_LOSS: True
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: True
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 512
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/MSRA/R-101
OUTPUT_DIR: data/relation_motif
PATHS_CATALOG: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.01
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 4
  MAX_ITER: 90000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (45000, 60000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 1
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: True
    SYNC_GATHER: False
  SAVE_PROPOSALS: False
2020-11-01 19:59:13,067 maskrcnn_benchmark INFO: Saving config into: data/relation_motif/config.yml
2020-11-01 19:59:13,099 maskrcnn_benchmark INFO: #################### prepare training ####################
2020-11-01 19:59:15,090 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-01 19:59:15,090 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2020-11-01 19:59:15,091 maskrcnn_benchmark.data.build INFO: Loading data statistics from: data/relation_motif/VG_stanford_filtered_with_attribute_train_statistics.cache
2020-11-01 19:59:15,091 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-01 19:59:24,386 maskrcnn_benchmark INFO: #################### end model construction ####################
2020-11-01 19:59:24,645 maskrcnn_benchmark INFO: #################### end optimizer and shcedule ####################
2020-11-01 19:59:24,649 maskrcnn_benchmark INFO: #################### end distributed ####################
2020-11-01 19:59:24,651 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from checkpoint/pretrained_faster_rcnn/model_final.pth
2020-11-01 19:59:25,068 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
2020-11-01 19:59:25,068 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
2020-11-01 19:59:25,068 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
2020-11-01 19:59:25,068 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
2020-11-01 19:59:25,068 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
2020-11-01 19:59:25,068 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
2020-11-01 19:59:25,068 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
2020-11-01 19:59:25,068 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
2020-11-01 19:59:25,068 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
2020-11-01 19:59:25,068 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
2020-11-01 19:59:25,195 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                                         loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
2020-11-01 19:59:25,195 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                                                       loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
2020-11-01 19:59:25,195 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                                         loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
2020-11-01 19:59:25,195 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                                                       loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
2020-11-01 19:59:25,196 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bi_freq_prior.weight of shape (1, 22801)
2020-11-01 19:59:25,196 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.0.bias of shape (128,)
2020-11-01 19:59:25,196 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.0.weight of shape (128, 9)
2020-11-01 19:59:25,196 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.bias of shape (128,)
2020-11-01 19:59:25,196 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.num_batches_tracked of shape ()
2020-11-01 19:59:25,196 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.running_mean of shape (128,)
2020-11-01 19:59:25,196 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.running_var of shape (128,)
2020-11-01 19:59:25,196 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.weight of shape (128,)
2020-11-01 19:59:25,197 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.embed_layer.weight of shape (152, 200)
2020-11-01 19:59:25,197 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.embed_out_layer.bias of shape (151,)
2020-11-01 19:59:25,197 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.embed_out_layer.weight of shape (151, 512)
2020-11-01 19:59:25,197 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.iofuh.bias of shape (2560,)
2020-11-01 19:59:25,197 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.iofuh.weight of shape (2560, 512)
2020-11-01 19:59:25,197 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.iofux.bias of shape (2560,)
2020-11-01 19:59:25,197 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.iofux.weight of shape (2560, 3088)
2020-11-01 19:59:25,197 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.px.bias of shape (512,)
2020-11-01 19:59:25,198 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.px.weight of shape (512, 3088)
2020-11-01 19:59:25,198 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.obj_embed.weight of shape (152, 200)
2020-11-01 19:59:25,198 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.out.bias of shape (151,)
2020-11-01 19:59:25,198 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.out.weight of shape (151, 512)
2020-11-01 19:59:25,198 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias of shape (1280,)
2020-11-01 19:59:25,198 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight of shape (1280, 256)
2020-11-01 19:59:25,198 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias of shape (1280,)
2020-11-01 19:59:25,198 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight of shape (1280, 2760)
2020-11-01 19:59:25,198 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias of shape (256,)
2020-11-01 19:59:25,199 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight of shape (256, 2760)
2020-11-01 19:59:25,199 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias of shape (1536,)
2020-11-01 19:59:25,199 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight of shape (1536, 256)
2020-11-01 19:59:25,199 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias of shape (1536,)
2020-11-01 19:59:25,199 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight of shape (1536, 256)
2020-11-01 19:59:25,199 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias of shape (1536,)
2020-11-01 19:59:25,199 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight of shape (1536, 2760)
2020-11-01 19:59:25,199 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias of shape (256,)
2020-11-01 19:59:25,199 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight of shape (256, 2760)
2020-11-01 19:59:25,199 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.emb_reduce.bias of shape (128,)
2020-11-01 19:59:25,200 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.emb_reduce.weight of shape (128, 200)
2020-11-01 19:59:25,200 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias of shape (1280,)
2020-11-01 19:59:25,200 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight of shape (1280, 256)
2020-11-01 19:59:25,200 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias of shape (1280,)
2020-11-01 19:59:25,200 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight of shape (1280, 2376)
2020-11-01 19:59:25,200 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias of shape (256,)
2020-11-01 19:59:25,200 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight of shape (256, 2376)
2020-11-01 19:59:25,200 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias of shape (1536,)
2020-11-01 19:59:25,200 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight of shape (1536, 256)
2020-11-01 19:59:25,201 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias of shape (1536,)
2020-11-01 19:59:25,201 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight of shape (1536, 256)
2020-11-01 19:59:25,201 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias of shape (1536,)
2020-11-01 19:59:25,201 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight of shape (1536, 2376)
2020-11-01 19:59:25,201 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias of shape (256,)
2020-11-01 19:59:25,201 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight of shape (256, 2376)
2020-11-01 19:59:25,201 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed1.weight of shape (151, 200)
2020-11-01 19:59:25,201 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed2.weight of shape (151, 200)
2020-11-01 19:59:25,201 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_reduce.bias of shape (128,)
2020-11-01 19:59:25,201 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_reduce.weight of shape (128, 2048)
2020-11-01 19:59:25,202 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.0.bias of shape (128,)
2020-11-01 19:59:25,202 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.0.weight of shape (128, 6)
2020-11-01 19:59:25,202 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.bias of shape (128,)
2020-11-01 19:59:25,202 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.num_batches_tracked of shape ()
2020-11-01 19:59:25,202 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.running_mean of shape (128,)
2020-11-01 19:59:25,202 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.running_var of shape (128,)
2020-11-01 19:59:25,202 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.weight of shape (128,)
2020-11-01 19:59:25,202 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.0.bias of shape (32,)
2020-11-01 19:59:25,202 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.0.weight of shape (32, 9)
2020-11-01 19:59:25,202 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.bias of shape (32,)
2020-11-01 19:59:25,203 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.num_batches_tracked of shape ()
2020-11-01 19:59:25,203 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.running_mean of shape (32,)
2020-11-01 19:59:25,203 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.running_var of shape (32,)
2020-11-01 19:59:25,203 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.weight of shape (32,)
2020-11-01 19:59:25,203 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.2.bias of shape (128,)
2020-11-01 19:59:25,203 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.2.weight of shape (128, 32)
2020-11-01 19:59:25,203 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_obj.bias of shape (512,)
2020-11-01 19:59:25,203 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_obj.weight of shape (512, 512)
2020-11-01 19:59:25,203 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_pre.bias of shape (512,)
2020-11-01 19:59:25,203 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_pre.weight of shape (512, 512)
2020-11-01 19:59:25,204 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_sub.bias of shape (512,)
2020-11-01 19:59:25,204 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_sub.weight of shape (512, 512)
2020-11-01 19:59:25,204 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.vision_prior.bias of shape (1,)
2020-11-01 19:59:25,204 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.vision_prior.weight of shape (1, 1537)
2020-11-01 19:59:25,204 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.bias of shape (51,)
2020-11-01 19:59:25,204 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.weight of shape (51, 4096)
2020-11-01 19:59:25,204 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.freq_bias.obj_baseline.weight of shape (22801, 51)
2020-11-01 19:59:25,204 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.bias of shape (4096,)
2020-11-01 19:59:25,204 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.weight of shape (4096, 1024)
2020-11-01 19:59:25,204 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.bias of shape (1024,)
2020-11-01 19:59:25,205 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.weight of shape (1024, 512)
2020-11-01 19:59:25,205 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.up_dim.bias of shape (4096,)
2020-11-01 19:59:25,205 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.up_dim.weight of shape (4096, 2048)
2020-11-01 19:59:25,205 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                                                     loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
2020-11-01 19:59:25,205 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                                                   loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
2020-11-01 19:59:25,205 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                                                     loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
2020-11-01 19:59:25,205 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                                                   loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
2020-11-01 19:59:25,205 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (512,)
2020-11-01 19:59:25,205 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (512, 2048, 3, 3)
2020-11-01 19:59:25,205 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.bias of shape (256,)
2020-11-01 19:59:25,206 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.weight of shape (256, 2, 7, 7)
2020-11-01 19:59:25,206 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.bias of shape (256,)
2020-11-01 19:59:25,206 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.num_batches_tracked of shape ()
2020-11-01 19:59:25,206 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_mean of shape (256,)
2020-11-01 19:59:25,206 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_var of shape (256,)
2020-11-01 19:59:25,206 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.weight of shape (256,)
2020-11-01 19:59:25,206 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.bias of shape (512,)
2020-11-01 19:59:25,206 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.weight of shape (512, 256, 3, 3)
2020-11-01 19:59:25,206 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.bias of shape (512,)
2020-11-01 19:59:25,206 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.num_batches_tracked of shape ()
2020-11-01 19:59:25,207 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_mean of shape (512,)
2020-11-01 19:59:25,207 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_var of shape (512,)
2020-11-01 19:59:25,207 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.weight of shape (512,)
2020-11-11 22:02:07,052 maskrcnn_benchmark INFO: Using 1 GPUs
2020-11-11 22:02:07,052 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_R_101_FPN_1x.yaml', distributed=False, local_rank=0, opts=['SOLVER.IMS_PER_BATCH', '4', 'TEST.IMS_PER_BATCH', '1', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '90000', 'SOLVER.STEPS', '(45000, 60000)', 'GLOVE_DIR', 'checkpoint/share/glove.6B', 'MODEL.PRETRAINED_DETECTOR_CKPT', 'checkpoint/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', 'data/relation_motif', 'SOLVER.PRE_VAL', 'False'], skip_test=False)
2020-11-11 22:02:07,053 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2020-11-11 22:02:08,495 maskrcnn_benchmark INFO: 
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.5 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: 
GPU 0: GeForce GTX 1650
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 455.32.00
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl  
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py36he904b0f_0  
[conda] mkl_fft                   1.2.0            py36h23d657b_0  
[conda] mkl_random                1.1.1            py36h0573a6f_0  
[conda] pytorch                   1.2.0           py3.6_cuda10.0.130_cudnn7.6.2_0    pytorch
[conda] torchvision               0.4.0                py36_cu100    pytorch
        Pillow (8.0.1)
2020-11-11 22:02:08,496 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_R_101_FPN_1x.yaml
2020-11-11 22:02:08,496 maskrcnn_benchmark INFO: 
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/MSRA/R-101"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 512
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 64
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 2048
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 64
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: True
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #PREDICTOR: "MotifPredictor"
    PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "CausalAnalysisPredictor"
    #PREDICTOR: "TransformerPredictor"
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    # Parameter for Transformer Module
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048 
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
OUTPUT_DIR: './output/relation_baseline'
TEST:
  RELATION:
    REQUIRE_OVERLAP: True
    LATER_NMS_PREDICTION_THRES: 0.5

2020-11-11 22:02:08,498 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GLOVE_DIR: checkpoint/share/glove.6B
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (800,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: checkpoint/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 1
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 64
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 2048
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 64
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: False
    BATCH_SIZE_PER_IMAGE: 64
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: False
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: False
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    LABEL_SMOOTHING_LOSS: True
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: True
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 512
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/MSRA/R-101
OUTPUT_DIR: data/relation_motif
PATHS_CATALOG: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.01
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 4
  MAX_ITER: 90000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (45000, 60000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 1
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: True
    SYNC_GATHER: False
  SAVE_PROPOSALS: False
2020-11-11 22:02:08,499 maskrcnn_benchmark INFO: Saving config into: data/relation_motif/config.yml
2020-11-11 22:02:08,614 maskrcnn_benchmark INFO: #################### prepare training ####################
2020-11-11 22:02:10,586 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-11 22:02:10,586 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2020-11-11 22:02:10,587 maskrcnn_benchmark.data.build INFO: Loading data statistics from: data/relation_motif/VG_stanford_filtered_with_attribute_train_statistics.cache
2020-11-11 22:02:10,587 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-11 22:02:23,628 maskrcnn_benchmark INFO: #################### end model construction ####################
2020-11-11 22:02:25,434 maskrcnn_benchmark INFO: #################### end optimizer and shcedule ####################
2020-11-11 22:02:25,439 maskrcnn_benchmark INFO: #################### end distributed ####################
2020-11-11 22:02:25,440 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from checkpoint/pretrained_faster_rcnn/model_final.pth
2020-11-11 22:02:29,196 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
2020-11-11 22:02:29,196 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
2020-11-11 22:02:29,196 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
2020-11-11 22:02:29,196 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
2020-11-11 22:02:29,197 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
2020-11-11 22:02:29,197 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
2020-11-11 22:02:29,197 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
2020-11-11 22:02:29,197 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
2020-11-11 22:02:29,197 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
2020-11-11 22:02:29,197 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
2020-11-11 22:02:29,419 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                                         loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
2020-11-11 22:02:29,420 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                                                       loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
2020-11-11 22:02:29,420 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                                         loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
2020-11-11 22:02:29,420 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                                                       loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
2020-11-11 22:02:29,420 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bi_freq_prior.weight of shape (1, 22801)
2020-11-11 22:02:29,420 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.0.bias of shape (128,)
2020-11-11 22:02:29,420 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.0.weight of shape (128, 9)
2020-11-11 22:02:29,420 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.bias of shape (128,)
2020-11-11 22:02:29,420 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.num_batches_tracked of shape ()
2020-11-11 22:02:29,421 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.running_mean of shape (128,)
2020-11-11 22:02:29,421 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.running_var of shape (128,)
2020-11-11 22:02:29,421 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.weight of shape (128,)
2020-11-11 22:02:29,421 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.embed_layer.weight of shape (152, 200)
2020-11-11 22:02:29,421 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.embed_out_layer.bias of shape (151,)
2020-11-11 22:02:29,421 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.embed_out_layer.weight of shape (151, 512)
2020-11-11 22:02:29,421 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.iofuh.bias of shape (2560,)
2020-11-11 22:02:29,421 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.iofuh.weight of shape (2560, 512)
2020-11-11 22:02:29,421 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.iofux.bias of shape (2560,)
2020-11-11 22:02:29,421 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.iofux.weight of shape (2560, 3088)
2020-11-11 22:02:29,421 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.px.bias of shape (512,)
2020-11-11 22:02:29,422 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.px.weight of shape (512, 3088)
2020-11-11 22:02:29,422 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.obj_embed.weight of shape (152, 200)
2020-11-11 22:02:29,422 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.out.bias of shape (151,)
2020-11-11 22:02:29,422 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.out.weight of shape (151, 512)
2020-11-11 22:02:29,422 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias of shape (1280,)
2020-11-11 22:02:29,422 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight of shape (1280, 256)
2020-11-11 22:02:29,422 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias of shape (1280,)
2020-11-11 22:02:29,422 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight of shape (1280, 2760)
2020-11-11 22:02:29,422 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias of shape (256,)
2020-11-11 22:02:29,422 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight of shape (256, 2760)
2020-11-11 22:02:29,423 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias of shape (1536,)
2020-11-11 22:02:29,423 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight of shape (1536, 256)
2020-11-11 22:02:29,423 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias of shape (1536,)
2020-11-11 22:02:29,423 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight of shape (1536, 256)
2020-11-11 22:02:29,423 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias of shape (1536,)
2020-11-11 22:02:29,423 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight of shape (1536, 2760)
2020-11-11 22:02:29,423 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias of shape (256,)
2020-11-11 22:02:29,423 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight of shape (256, 2760)
2020-11-11 22:02:29,423 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.emb_reduce.bias of shape (128,)
2020-11-11 22:02:29,423 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.emb_reduce.weight of shape (128, 200)
2020-11-11 22:02:29,424 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias of shape (1280,)
2020-11-11 22:02:29,424 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight of shape (1280, 256)
2020-11-11 22:02:29,424 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias of shape (1280,)
2020-11-11 22:02:29,424 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight of shape (1280, 2376)
2020-11-11 22:02:29,424 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias of shape (256,)
2020-11-11 22:02:29,424 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight of shape (256, 2376)
2020-11-11 22:02:29,424 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias of shape (1536,)
2020-11-11 22:02:29,424 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight of shape (1536, 256)
2020-11-11 22:02:29,424 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias of shape (1536,)
2020-11-11 22:02:29,424 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight of shape (1536, 256)
2020-11-11 22:02:29,424 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias of shape (1536,)
2020-11-11 22:02:29,425 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight of shape (1536, 2376)
2020-11-11 22:02:29,425 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias of shape (256,)
2020-11-11 22:02:29,425 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight of shape (256, 2376)
2020-11-11 22:02:29,425 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed1.weight of shape (151, 200)
2020-11-11 22:02:29,425 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed2.weight of shape (151, 200)
2020-11-11 22:02:29,425 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_reduce.bias of shape (128,)
2020-11-11 22:02:29,425 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_reduce.weight of shape (128, 2048)
2020-11-11 22:02:29,425 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.0.bias of shape (128,)
2020-11-11 22:02:29,425 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.0.weight of shape (128, 6)
2020-11-11 22:02:29,425 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.bias of shape (128,)
2020-11-11 22:02:29,425 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.num_batches_tracked of shape ()
2020-11-11 22:02:29,426 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.running_mean of shape (128,)
2020-11-11 22:02:29,426 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.running_var of shape (128,)
2020-11-11 22:02:29,426 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.weight of shape (128,)
2020-11-11 22:02:29,426 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.0.bias of shape (32,)
2020-11-11 22:02:29,426 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.0.weight of shape (32, 9)
2020-11-11 22:02:29,426 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.bias of shape (32,)
2020-11-11 22:02:29,426 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.num_batches_tracked of shape ()
2020-11-11 22:02:29,426 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.running_mean of shape (32,)
2020-11-11 22:02:29,426 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.running_var of shape (32,)
2020-11-11 22:02:29,426 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.weight of shape (32,)
2020-11-11 22:02:29,426 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.2.bias of shape (128,)
2020-11-11 22:02:29,427 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.2.weight of shape (128, 32)
2020-11-11 22:02:29,427 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_obj.bias of shape (512,)
2020-11-11 22:02:29,427 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_obj.weight of shape (512, 512)
2020-11-11 22:02:29,427 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_pre.bias of shape (512,)
2020-11-11 22:02:29,427 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_pre.weight of shape (512, 512)
2020-11-11 22:02:29,427 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_sub.bias of shape (512,)
2020-11-11 22:02:29,427 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_sub.weight of shape (512, 512)
2020-11-11 22:02:29,427 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.vision_prior.bias of shape (1,)
2020-11-11 22:02:29,427 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.vision_prior.weight of shape (1, 1537)
2020-11-11 22:02:29,427 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.bias of shape (51,)
2020-11-11 22:02:29,427 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.weight of shape (51, 4096)
2020-11-11 22:02:29,427 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.freq_bias.obj_baseline.weight of shape (22801, 51)
2020-11-11 22:02:29,428 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.bias of shape (4096,)
2020-11-11 22:02:29,428 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.weight of shape (4096, 1024)
2020-11-11 22:02:29,428 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.bias of shape (1024,)
2020-11-11 22:02:29,428 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.weight of shape (1024, 512)
2020-11-11 22:02:29,428 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.up_dim.bias of shape (4096,)
2020-11-11 22:02:29,428 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.up_dim.weight of shape (4096, 2048)
2020-11-11 22:02:29,428 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                                                     loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
2020-11-11 22:02:29,428 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                                                   loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
2020-11-11 22:02:29,428 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                                                     loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
2020-11-11 22:02:29,428 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                                                   loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
2020-11-11 22:02:29,429 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (512,)
2020-11-11 22:02:29,429 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (512, 2048, 3, 3)
2020-11-11 22:02:29,429 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.bias of shape (256,)
2020-11-11 22:02:29,429 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.weight of shape (256, 2, 7, 7)
2020-11-11 22:02:29,429 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.bias of shape (256,)
2020-11-11 22:02:29,429 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.num_batches_tracked of shape ()
2020-11-11 22:02:29,429 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_mean of shape (256,)
2020-11-11 22:02:29,429 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_var of shape (256,)
2020-11-11 22:02:29,429 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.weight of shape (256,)
2020-11-11 22:02:29,429 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.bias of shape (512,)
2020-11-11 22:02:29,429 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.weight of shape (512, 256, 3, 3)
2020-11-11 22:02:29,429 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.bias of shape (512,)
2020-11-11 22:02:29,430 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.num_batches_tracked of shape ()
2020-11-11 22:02:29,430 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_mean of shape (512,)
2020-11-11 22:02:29,430 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_var of shape (512,)
2020-11-11 22:02:29,430 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.weight of shape (512,)
2020-11-11 22:03:44,838 maskrcnn_benchmark INFO: Using 1 GPUs
2020-11-11 22:03:44,838 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_R_101_FPN_1x.yaml', distributed=False, local_rank=0, opts=['SOLVER.IMS_PER_BATCH', '2', 'TEST.IMS_PER_BATCH', '1', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '90000', 'SOLVER.STEPS', '(45000, 60000)', 'GLOVE_DIR', 'checkpoint/share/glove.6B', 'MODEL.PRETRAINED_DETECTOR_CKPT', 'checkpoint/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', 'data/relation_motif', 'SOLVER.PRE_VAL', 'False'], skip_test=False)
2020-11-11 22:03:44,838 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2020-11-11 22:03:46,296 maskrcnn_benchmark INFO: 
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.5 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: 
GPU 0: GeForce GTX 1650
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 455.32.00
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl  
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py36he904b0f_0  
[conda] mkl_fft                   1.2.0            py36h23d657b_0  
[conda] mkl_random                1.1.1            py36h0573a6f_0  
[conda] pytorch                   1.2.0           py3.6_cuda10.0.130_cudnn7.6.2_0    pytorch
[conda] torchvision               0.4.0                py36_cu100    pytorch
        Pillow (8.0.1)
2020-11-11 22:03:46,296 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_R_101_FPN_1x.yaml
2020-11-11 22:03:46,297 maskrcnn_benchmark INFO: 
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/MSRA/R-101"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 512
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 64
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 2048
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 64
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: True
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #PREDICTOR: "MotifPredictor"
    PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "CausalAnalysisPredictor"
    #PREDICTOR: "TransformerPredictor"
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    # Parameter for Transformer Module
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048 
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
OUTPUT_DIR: './output/relation_baseline'
TEST:
  RELATION:
    REQUIRE_OVERLAP: True
    LATER_NMS_PREDICTION_THRES: 0.5

2020-11-11 22:03:46,299 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GLOVE_DIR: checkpoint/share/glove.6B
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (800,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: checkpoint/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 1
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 64
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 2048
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 64
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: False
    BATCH_SIZE_PER_IMAGE: 64
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: False
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: False
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    LABEL_SMOOTHING_LOSS: True
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: True
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 512
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/MSRA/R-101
OUTPUT_DIR: data/relation_motif
PATHS_CATALOG: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.01
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 2
  MAX_ITER: 90000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (45000, 60000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 1
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: True
    SYNC_GATHER: False
  SAVE_PROPOSALS: False
2020-11-11 22:03:46,299 maskrcnn_benchmark INFO: Saving config into: data/relation_motif/config.yml
2020-11-11 22:03:46,333 maskrcnn_benchmark INFO: #################### prepare training ####################
2020-11-11 22:03:48,318 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-11 22:03:48,318 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2020-11-11 22:03:48,318 maskrcnn_benchmark.data.build INFO: Loading data statistics from: data/relation_motif/VG_stanford_filtered_with_attribute_train_statistics.cache
2020-11-11 22:03:48,318 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-11 22:03:56,560 maskrcnn_benchmark INFO: #################### end model construction ####################
2020-11-11 22:03:58,051 maskrcnn_benchmark INFO: #################### end optimizer and shcedule ####################
2020-11-11 22:03:58,056 maskrcnn_benchmark INFO: #################### end distributed ####################
2020-11-11 22:03:58,057 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from checkpoint/pretrained_faster_rcnn/model_final.pth
2020-11-11 22:03:58,461 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
2020-11-11 22:03:58,461 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
2020-11-11 22:03:58,461 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
2020-11-11 22:03:58,461 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
2020-11-11 22:03:58,461 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
2020-11-11 22:03:58,461 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
2020-11-11 22:03:58,461 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
2020-11-11 22:03:58,461 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
2020-11-11 22:03:58,461 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
2020-11-11 22:03:58,461 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
2020-11-11 22:03:58,503 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                                         loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
2020-11-11 22:03:58,503 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                                                       loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
2020-11-11 22:03:58,503 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                                         loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
2020-11-11 22:03:58,503 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                                                       loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
2020-11-11 22:03:58,503 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bi_freq_prior.weight of shape (1, 22801)
2020-11-11 22:03:58,503 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.0.bias of shape (128,)
2020-11-11 22:03:58,503 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.0.weight of shape (128, 9)
2020-11-11 22:03:58,503 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.bias of shape (128,)
2020-11-11 22:03:58,503 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.num_batches_tracked of shape ()
2020-11-11 22:03:58,503 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.running_mean of shape (128,)
2020-11-11 22:03:58,503 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.running_var of shape (128,)
2020-11-11 22:03:58,503 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.box_embed.1.weight of shape (128,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.embed_layer.weight of shape (152, 200)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.embed_out_layer.bias of shape (151,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.embed_out_layer.weight of shape (151, 512)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.iofuh.bias of shape (2560,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.iofuh.weight of shape (2560, 512)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.iofux.bias of shape (2560,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.iofux.weight of shape (2560, 3088)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.px.bias of shape (512,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.decoderLSTM.px.weight of shape (512, 3088)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.obj_embed.weight of shape (152, 200)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.out.bias of shape (151,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.decoder_rnn.out.weight of shape (151, 512)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias of shape (1280,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight of shape (1280, 256)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias of shape (1280,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight of shape (1280, 2760)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias of shape (256,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight of shape (256, 2760)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias of shape (1536,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight of shape (1536, 256)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias of shape (1536,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight of shape (1536, 256)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias of shape (1536,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight of shape (1536, 2760)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias of shape (256,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight of shape (256, 2760)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.emb_reduce.bias of shape (128,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.emb_reduce.weight of shape (128, 200)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias of shape (1280,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight of shape (1280, 256)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias of shape (1280,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight of shape (1280, 2376)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias of shape (256,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight of shape (256, 2376)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias of shape (1536,)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight of shape (1536, 256)
2020-11-11 22:03:58,504 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias of shape (1536,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight of shape (1536, 256)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias of shape (1536,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight of shape (1536, 2376)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias of shape (256,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight of shape (256, 2376)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed1.weight of shape (151, 200)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed2.weight of shape (151, 200)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_reduce.bias of shape (128,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_reduce.weight of shape (128, 2048)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.0.bias of shape (128,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.0.weight of shape (128, 6)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.bias of shape (128,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.num_batches_tracked of shape ()
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.running_mean of shape (128,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.running_var of shape (128,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.overlap_embed.1.weight of shape (128,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.0.bias of shape (32,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.0.weight of shape (32, 9)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.bias of shape (32,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.num_batches_tracked of shape ()
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.running_mean of shape (32,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.running_var of shape (32,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.1.weight of shape (32,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.2.bias of shape (128,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.pos_embed.2.weight of shape (128, 32)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_obj.bias of shape (512,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_obj.weight of shape (512, 512)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_pre.bias of shape (512,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_pre.weight of shape (512, 512)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_sub.bias of shape (512,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.score_sub.weight of shape (512, 512)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.vision_prior.bias of shape (1,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.vision_prior.weight of shape (1, 1537)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.bias of shape (51,)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.weight of shape (51, 4096)
2020-11-11 22:03:58,505 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.freq_bias.obj_baseline.weight of shape (22801, 51)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.bias of shape (4096,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.weight of shape (4096, 1024)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.bias of shape (1024,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.weight of shape (1024, 512)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.up_dim.bias of shape (4096,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.up_dim.weight of shape (4096, 2048)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                                                     loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                                                   loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                                                     loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                                                   loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (512,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (512, 2048, 3, 3)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.bias of shape (256,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.weight of shape (256, 2, 7, 7)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.bias of shape (256,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.num_batches_tracked of shape ()
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_mean of shape (256,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_var of shape (256,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.weight of shape (256,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.bias of shape (512,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.weight of shape (512, 256, 3, 3)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.bias of shape (512,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.num_batches_tracked of shape ()
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_mean of shape (512,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_var of shape (512,)
2020-11-11 22:03:58,506 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.weight of shape (512,)
2020-11-11 22:10:39,020 maskrcnn_benchmark INFO: Using 1 GPUs
2020-11-11 22:10:39,020 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_R_101_FPN_1x.yaml', distributed=False, local_rank=0, opts=['SOLVER.IMS_PER_BATCH', '2', 'TEST.IMS_PER_BATCH', '1', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '90000', 'SOLVER.STEPS', '(45000, 60000)', 'GLOVE_DIR', 'checkpoint/share/glove.6B', 'MODEL.PRETRAINED_DETECTOR_CKPT', 'checkpoint/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', 'data/relation_motif', 'SOLVER.PRE_VAL', 'False'], skip_test=False)
2020-11-11 22:10:39,020 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2020-11-11 22:10:40,441 maskrcnn_benchmark INFO: 
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.5 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: 
GPU 0: GeForce GTX 1650
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 455.32.00
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl  
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py36he904b0f_0  
[conda] mkl_fft                   1.2.0            py36h23d657b_0  
[conda] mkl_random                1.1.1            py36h0573a6f_0  
[conda] pytorch                   1.2.0           py3.6_cuda10.0.130_cudnn7.6.2_0    pytorch
[conda] torchvision               0.4.0                py36_cu100    pytorch
        Pillow (8.0.1)
2020-11-11 22:10:40,441 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_R_101_FPN_1x.yaml
2020-11-11 22:10:40,442 maskrcnn_benchmark INFO: 
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/MSRA/R-101"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 512
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 64
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 2048
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 64
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: True
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #PREDICTOR: "MotifPredictor"
    PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "CausalAnalysisPredictor"
    #PREDICTOR: "TransformerPredictor"
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    # Parameter for Transformer Module
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048 
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
OUTPUT_DIR: './output/relation_baseline'
TEST:
  RELATION:
    REQUIRE_OVERLAP: True
    LATER_NMS_PREDICTION_THRES: 0.5

2020-11-11 22:10:40,444 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GLOVE_DIR: checkpoint/share/glove.6B
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (800,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: checkpoint/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 1
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 64
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 2048
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 64
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: False
    BATCH_SIZE_PER_IMAGE: 64
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: False
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: False
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    LABEL_SMOOTHING_LOSS: True
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: True
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 512
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/MSRA/R-101
OUTPUT_DIR: data/relation_motif
PATHS_CATALOG: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.01
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 2
  MAX_ITER: 90000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (45000, 60000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 1
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: True
    SYNC_GATHER: False
  SAVE_PROPOSALS: False
2020-11-11 22:10:40,444 maskrcnn_benchmark INFO: Saving config into: data/relation_motif/config.yml
2020-11-11 22:10:40,478 maskrcnn_benchmark INFO: #################### prepare training ####################
2020-11-11 22:10:42,449 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-11 22:10:42,450 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2020-11-11 22:10:42,450 maskrcnn_benchmark.data.build INFO: Loading data statistics from: data/relation_motif/VG_stanford_filtered_with_attribute_train_statistics.cache
2020-11-11 22:10:42,450 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-11 22:10:50,782 maskrcnn_benchmark INFO: #################### end model construction ####################
2020-11-11 22:10:52,282 maskrcnn_benchmark INFO: #################### end optimizer and shcedule ####################
2020-11-11 22:10:52,287 maskrcnn_benchmark INFO: #################### end distributed ####################
2020-11-11 22:10:52,288 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from checkpoint/pretrained_faster_rcnn/model_final.pth
2020-11-11 22:10:52,729 maskrcnn_benchmark INFO: #################### end load checkpointer ####################
2020-11-11 22:10:52,729 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2020-11-11 22:10:54,565 maskrcnn_benchmark.utils.miscellaneous INFO: Saving labels mapping into data/relation_motif/labels.json
2020-11-11 22:10:55,388 maskrcnn_benchmark INFO: #################### end dataloader ####################
2020-11-11 22:10:55,388 maskrcnn_benchmark INFO: Start training
2020-11-11 22:11:02,186 maskrcnn_benchmark INFO: ---Total norm nan clip coef nan-----------------
2020-11-11 22:11:02,191 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: nan, (torch.Size([512, 2048, 3, 3]))
2020-11-11 22:11:02,191 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: nan, (torch.Size([512]))
2020-11-11 22:11:02,191 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: nan, (torch.Size([2048, 25088]))
2020-11-11 22:11:02,191 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: nan, (torch.Size([2048]))
2020-11-11 22:11:02,191 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: nan, (torch.Size([2048, 2048]))
2020-11-11 22:11:02,191 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: nan, (torch.Size([2048]))
2020-11-11 22:11:02,191 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.weight: nan, (torch.Size([256, 2, 7, 7]))
2020-11-11 22:11:02,191 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.bias: nan, (torch.Size([256]))
2020-11-11 22:11:02,191 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.weight: nan, (torch.Size([256]))
2020-11-11 22:11:02,191 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.bias: nan, (torch.Size([256]))
2020-11-11 22:11:02,191 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.weight: nan, (torch.Size([512, 256, 3, 3]))
2020-11-11 22:11:02,191 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.bias: nan, (torch.Size([512]))
2020-11-11 22:11:02,191 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.weight: nan, (torch.Size([512]))
2020-11-11 22:11:02,191 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.bias: nan, (torch.Size([512]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([2048, 25088]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([2048]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([2048, 2048]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([2048]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed1.weight: nan, (torch.Size([151, 200]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed2.weight: nan, (torch.Size([151, 200]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.weight: nan, (torch.Size([32, 9]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.bias: nan, (torch.Size([32]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.weight: nan, (torch.Size([32]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.bias: nan, (torch.Size([32]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.weight: nan, (torch.Size([128, 32]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.bias: nan, (torch.Size([128]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.weight: nan, (torch.Size([128, 6]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.bias: nan, (torch.Size([128]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.weight: nan, (torch.Size([128]))
2020-11-11 22:11:02,192 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.bias: nan, (torch.Size([128]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.weight: nan, (torch.Size([128, 9]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.bias: nan, (torch.Size([128]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.weight: nan, (torch.Size([128]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.bias: nan, (torch.Size([128]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bi_freq_prior.weight: nan, (torch.Size([1, 22801]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.weight: nan, (torch.Size([128, 2048]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.bias: nan, (torch.Size([128]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.weight: nan, (torch.Size([128, 200]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.bias: nan, (torch.Size([128]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.weight: nan, (torch.Size([512, 512]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.bias: nan, (torch.Size([512]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.weight: nan, (torch.Size([512, 512]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.bias: nan, (torch.Size([512]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.weight: nan, (torch.Size([512, 512]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.bias: nan, (torch.Size([512]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.weight: nan, (torch.Size([1, 1537]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.bias: nan, (torch.Size([1]))
2020-11-11 22:11:02,193 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: nan, (torch.Size([256, 2376]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: nan, (torch.Size([1536, 2376]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: nan, (torch.Size([1536]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: nan, (torch.Size([1536, 256]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: nan, (torch.Size([1536]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: nan, (torch.Size([1536, 256]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: nan, (torch.Size([1536]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: nan, (torch.Size([256, 2376]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: nan, (torch.Size([1280, 2376]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: nan, (torch.Size([1280]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: nan, (torch.Size([1280, 256]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: nan, (torch.Size([1280]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: nan, (torch.Size([256, 2760]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: nan, (torch.Size([1536, 2760]))
2020-11-11 22:11:02,194 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: nan, (torch.Size([1536]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: nan, (torch.Size([1536, 256]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: nan, (torch.Size([1536]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: nan, (torch.Size([1536, 256]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: nan, (torch.Size([1536]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: nan, (torch.Size([256, 2760]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: nan, (torch.Size([1280, 2760]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: nan, (torch.Size([1280]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: nan, (torch.Size([1280, 256]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: nan, (torch.Size([1280]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.weight      : nan, (torch.Size([1024, 512]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.bias        : nan, (torch.Size([1024]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.weight      : nan, (torch.Size([4096, 1024]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.bias        : nan, (torch.Size([4096]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.weight  : nan, (torch.Size([51, 4096]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.bias    : nan, (torch.Size([51]))
2020-11-11 22:11:02,195 maskrcnn_benchmark INFO: roi_heads.relation.predictor.up_dim.weight        : nan, (torch.Size([4096, 2048]))
2020-11-11 22:11:02,196 maskrcnn_benchmark INFO: roi_heads.relation.predictor.up_dim.bias          : nan, (torch.Size([4096]))
2020-11-11 22:11:02,196 maskrcnn_benchmark INFO: roi_heads.relation.predictor.freq_bias.obj_baseline.weight: nan, (torch.Size([22801, 51]))
2020-11-11 22:11:02,196 maskrcnn_benchmark INFO: -------------------------------
2020-11-11 22:20:30,812 maskrcnn_benchmark INFO: Using 1 GPUs
2020-11-11 22:20:30,812 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_R_101_FPN_1x.yaml', distributed=False, local_rank=0, opts=['SOLVER.IMS_PER_BATCH', '4', 'TEST.IMS_PER_BATCH', '1', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '90000', 'SOLVER.STEPS', '(45000, 60000)', 'GLOVE_DIR', 'checkpoint/share/glove.6B', 'MODEL.PRETRAINED_DETECTOR_CKPT', 'checkpoint/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', 'data/relation_motif', 'SOLVER.PRE_VAL', 'False'], skip_test=False)
2020-11-11 22:20:30,812 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2020-11-11 22:20:32,256 maskrcnn_benchmark INFO: 
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.5 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: 
GPU 0: GeForce GTX 1650
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 455.32.00
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl  
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py36he904b0f_0  
[conda] mkl_fft                   1.2.0            py36h23d657b_0  
[conda] mkl_random                1.1.1            py36h0573a6f_0  
[conda] pytorch                   1.2.0           py3.6_cuda10.0.130_cudnn7.6.2_0    pytorch
[conda] torchvision               0.4.0                py36_cu100    pytorch
        Pillow (8.0.1)
2020-11-11 22:20:32,257 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_R_101_FPN_1x.yaml
2020-11-11 22:20:32,257 maskrcnn_benchmark INFO: 
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/MSRA/R-101"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 512
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 64
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 2048
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 64
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: True
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #PREDICTOR: "MotifPredictor"
    PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "CausalAnalysisPredictor"
    #PREDICTOR: "TransformerPredictor"
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    # Parameter for Transformer Module
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048 
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
OUTPUT_DIR: './output/relation_baseline'
TEST:
  RELATION:
    REQUIRE_OVERLAP: True
    LATER_NMS_PREDICTION_THRES: 0.5

2020-11-11 22:20:32,259 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GLOVE_DIR: checkpoint/share/glove.6B
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (800,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: checkpoint/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 1
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 64
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 2048
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 64
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: False
    BATCH_SIZE_PER_IMAGE: 64
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: False
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: False
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    LABEL_SMOOTHING_LOSS: True
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: True
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 512
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/MSRA/R-101
OUTPUT_DIR: data/relation_motif
PATHS_CATALOG: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.01
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 4
  MAX_ITER: 90000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (45000, 60000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 1
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: True
    SYNC_GATHER: False
  SAVE_PROPOSALS: False
2020-11-11 22:20:32,260 maskrcnn_benchmark INFO: Saving config into: data/relation_motif/config.yml
2020-11-11 22:20:32,293 maskrcnn_benchmark INFO: #################### prepare training ####################
2020-11-11 22:20:34,264 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-11 22:20:34,264 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2020-11-11 22:20:34,264 maskrcnn_benchmark.data.build INFO: Loading data statistics from: data/relation_motif/VG_stanford_filtered_with_attribute_train_statistics.cache
2020-11-11 22:20:34,264 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-11 22:20:43,526 maskrcnn_benchmark INFO: #################### end model construction ####################
2020-11-11 22:20:43,773 maskrcnn_benchmark INFO: #################### end optimizer and shcedule ####################
2020-11-11 22:20:43,777 maskrcnn_benchmark INFO: #################### end distributed ####################
2020-11-11 22:20:43,778 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from checkpoint/pretrained_faster_rcnn/model_final.pth
2020-11-11 22:20:44,210 maskrcnn_benchmark INFO: #################### end load checkpointer ####################
2020-11-11 22:20:44,210 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2020-11-11 22:20:46,032 maskrcnn_benchmark.utils.miscellaneous INFO: Saving labels mapping into data/relation_motif/labels.json
2020-11-11 22:20:46,834 maskrcnn_benchmark INFO: #################### end dataloader ####################
2020-11-11 22:20:46,834 maskrcnn_benchmark INFO: Start training
2020-11-11 22:20:48,970 maskrcnn_benchmark INFO: ---Total norm nan clip coef nan-----------------
2020-11-11 22:20:48,975 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: nan, (torch.Size([512, 2048, 3, 3]))
2020-11-11 22:20:48,975 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: nan, (torch.Size([512]))
2020-11-11 22:20:48,975 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: nan, (torch.Size([2048, 25088]))
2020-11-11 22:20:48,975 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: nan, (torch.Size([2048]))
2020-11-11 22:20:48,975 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: nan, (torch.Size([2048, 2048]))
2020-11-11 22:20:48,975 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: nan, (torch.Size([2048]))
2020-11-11 22:20:48,975 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.weight: nan, (torch.Size([256, 2, 7, 7]))
2020-11-11 22:20:48,975 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.bias: nan, (torch.Size([256]))
2020-11-11 22:20:48,975 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.weight: nan, (torch.Size([256]))
2020-11-11 22:20:48,975 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.bias: inf, (torch.Size([256]))
2020-11-11 22:20:48,975 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.weight: nan, (torch.Size([512, 256, 3, 3]))
2020-11-11 22:20:48,975 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.bias: nan, (torch.Size([512]))
2020-11-11 22:20:48,975 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.weight: nan, (torch.Size([512]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.bias: nan, (torch.Size([512]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([2048, 25088]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([2048]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([2048, 2048]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([2048]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed1.weight: nan, (torch.Size([151, 200]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed2.weight: nan, (torch.Size([151, 200]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.weight: nan, (torch.Size([32, 9]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.bias: nan, (torch.Size([32]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.weight: nan, (torch.Size([32]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.bias: nan, (torch.Size([32]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.weight: nan, (torch.Size([128, 32]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.bias: nan, (torch.Size([128]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.weight: nan, (torch.Size([128, 6]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.bias: nan, (torch.Size([128]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.weight: nan, (torch.Size([128]))
2020-11-11 22:20:48,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.bias: nan, (torch.Size([128]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.weight: nan, (torch.Size([128, 9]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.bias: nan, (torch.Size([128]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.weight: nan, (torch.Size([128]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.bias: nan, (torch.Size([128]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bi_freq_prior.weight: nan, (torch.Size([1, 22801]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.weight: nan, (torch.Size([128, 2048]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.bias: nan, (torch.Size([128]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.weight: nan, (torch.Size([128, 200]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.bias: nan, (torch.Size([128]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.weight: nan, (torch.Size([512, 512]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.bias: nan, (torch.Size([512]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.weight: nan, (torch.Size([512, 512]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.bias: nan, (torch.Size([512]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.weight: nan, (torch.Size([512, 512]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.bias: nan, (torch.Size([512]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.weight: nan, (torch.Size([1, 1537]))
2020-11-11 22:20:48,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.bias: nan, (torch.Size([1]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: nan, (torch.Size([256, 2376]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: nan, (torch.Size([1536, 2376]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: nan, (torch.Size([1536]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: nan, (torch.Size([1536, 256]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: nan, (torch.Size([1536]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: nan, (torch.Size([1536, 256]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: nan, (torch.Size([1536]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: nan, (torch.Size([256, 2376]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: nan, (torch.Size([1280, 2376]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: nan, (torch.Size([1280]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: nan, (torch.Size([1280, 256]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: nan, (torch.Size([1280]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: nan, (torch.Size([256, 2760]))
2020-11-11 22:20:48,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: nan, (torch.Size([1536, 2760]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: nan, (torch.Size([1536]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: nan, (torch.Size([1536, 256]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: nan, (torch.Size([1536]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: nan, (torch.Size([1536, 256]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: nan, (torch.Size([1536]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: nan, (torch.Size([256, 2760]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: nan, (torch.Size([1280, 2760]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: nan, (torch.Size([1280]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: nan, (torch.Size([1280, 256]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: nan, (torch.Size([1280]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.weight      : nan, (torch.Size([1024, 512]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.bias        : nan, (torch.Size([1024]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.weight      : nan, (torch.Size([4096, 1024]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.bias        : nan, (torch.Size([4096]))
2020-11-11 22:20:48,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.weight  : nan, (torch.Size([51, 4096]))
2020-11-11 22:20:48,980 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.bias    : nan, (torch.Size([51]))
2020-11-11 22:20:48,980 maskrcnn_benchmark INFO: roi_heads.relation.predictor.up_dim.weight        : nan, (torch.Size([4096, 2048]))
2020-11-11 22:20:48,980 maskrcnn_benchmark INFO: roi_heads.relation.predictor.up_dim.bias          : nan, (torch.Size([4096]))
2020-11-11 22:20:48,980 maskrcnn_benchmark INFO: roi_heads.relation.predictor.freq_bias.obj_baseline.weight: nan, (torch.Size([22801, 51]))
2020-11-11 22:20:48,980 maskrcnn_benchmark INFO: -------------------------------
2020-11-11 22:28:32,377 maskrcnn_benchmark INFO: Using 1 GPUs
2020-11-11 22:28:32,377 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_R_101_FPN_1x.yaml', distributed=False, local_rank=0, opts=['SOLVER.IMS_PER_BATCH', '4', 'TEST.IMS_PER_BATCH', '1', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '90000', 'SOLVER.STEPS', '(45000, 60000)', 'GLOVE_DIR', 'checkpoint/share/glove.6B', 'MODEL.PRETRAINED_DETECTOR_CKPT', 'checkpoint/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', 'data/relation_motif', 'SOLVER.PRE_VAL', 'False'], skip_test=False)
2020-11-11 22:28:32,377 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2020-11-11 22:28:33,786 maskrcnn_benchmark INFO: 
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.5 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: 
GPU 0: GeForce GTX 1650
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 455.32.00
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl  
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py36he904b0f_0  
[conda] mkl_fft                   1.2.0            py36h23d657b_0  
[conda] mkl_random                1.1.1            py36h0573a6f_0  
[conda] pytorch                   1.2.0           py3.6_cuda10.0.130_cudnn7.6.2_0    pytorch
[conda] torchvision               0.4.0                py36_cu100    pytorch
        Pillow (8.0.1)
2020-11-11 22:28:33,787 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_R_101_FPN_1x.yaml
2020-11-11 22:28:33,787 maskrcnn_benchmark INFO: 
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/MSRA/R-101"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 512
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 64
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 2048
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 64
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: True
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #PREDICTOR: "MotifPredictor"
    PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "CausalAnalysisPredictor"
    #PREDICTOR: "TransformerPredictor"
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    # Parameter for Transformer Module
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048 
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
OUTPUT_DIR: './output/relation_baseline'
TEST:
  RELATION:
    REQUIRE_OVERLAP: True
    LATER_NMS_PREDICTION_THRES: 0.5

2020-11-11 22:28:33,789 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GLOVE_DIR: checkpoint/share/glove.6B
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (800,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: checkpoint/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 1
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 64
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 2048
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 64
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: False
    BATCH_SIZE_PER_IMAGE: 64
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: False
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: False
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    LABEL_SMOOTHING_LOSS: True
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: True
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 512
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/MSRA/R-101
OUTPUT_DIR: data/relation_motif
PATHS_CATALOG: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.01
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 4
  MAX_ITER: 90000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (45000, 60000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 1
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: True
    SYNC_GATHER: False
  SAVE_PROPOSALS: False
2020-11-11 22:28:33,789 maskrcnn_benchmark INFO: Saving config into: data/relation_motif/config.yml
2020-11-11 22:28:33,822 maskrcnn_benchmark INFO: #################### prepare training ####################
2020-11-11 22:28:35,847 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-11 22:28:35,847 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2020-11-11 22:28:35,848 maskrcnn_benchmark.data.build INFO: Loading data statistics from: data/relation_motif/VG_stanford_filtered_with_attribute_train_statistics.cache
2020-11-11 22:28:35,848 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-11 22:28:43,202 maskrcnn_benchmark INFO: #################### end model construction ####################
2020-11-11 22:28:43,446 maskrcnn_benchmark INFO: #################### end optimizer and shcedule ####################
2020-11-11 22:28:43,451 maskrcnn_benchmark INFO: #################### end distributed ####################
2020-11-11 22:28:43,452 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from checkpoint/pretrained_faster_rcnn/model_final.pth
2020-11-11 22:28:43,886 maskrcnn_benchmark INFO: #################### end load checkpointer ####################
2020-11-11 22:28:43,887 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2020-11-11 22:28:45,651 maskrcnn_benchmark.utils.miscellaneous INFO: Saving labels mapping into data/relation_motif/labels.json
2020-11-11 22:28:46,354 maskrcnn_benchmark INFO: #################### end dataloader ####################
2020-11-11 22:28:46,355 maskrcnn_benchmark INFO: Start training
2020-11-11 22:28:48,205 maskrcnn_benchmark INFO: ---Total norm nan clip coef nan-----------------
2020-11-11 22:28:48,210 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: nan, (torch.Size([512, 2048, 3, 3]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: nan, (torch.Size([512]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: nan, (torch.Size([2048, 25088]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: nan, (torch.Size([2048]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: nan, (torch.Size([2048, 2048]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: nan, (torch.Size([2048]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.weight: nan, (torch.Size([256, 2, 7, 7]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.bias: nan, (torch.Size([256]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.weight: nan, (torch.Size([256]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.bias: inf, (torch.Size([256]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.weight: nan, (torch.Size([512, 256, 3, 3]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.bias: nan, (torch.Size([512]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.weight: nan, (torch.Size([512]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.bias: nan, (torch.Size([512]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([2048, 25088]))
2020-11-11 22:28:48,211 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([2048]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([2048, 2048]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([2048]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed1.weight: nan, (torch.Size([151, 200]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed2.weight: nan, (torch.Size([151, 200]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.weight: nan, (torch.Size([32, 9]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.bias: nan, (torch.Size([32]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.weight: nan, (torch.Size([32]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.bias: nan, (torch.Size([32]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.weight: nan, (torch.Size([128, 32]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.bias: nan, (torch.Size([128]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.weight: nan, (torch.Size([128, 6]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.bias: nan, (torch.Size([128]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.weight: nan, (torch.Size([128]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.bias: nan, (torch.Size([128]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.weight: nan, (torch.Size([128, 9]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.bias: nan, (torch.Size([128]))
2020-11-11 22:28:48,212 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.weight: nan, (torch.Size([128]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.bias: nan, (torch.Size([128]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bi_freq_prior.weight: nan, (torch.Size([1, 22801]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.weight: nan, (torch.Size([128, 2048]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.bias: nan, (torch.Size([128]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.weight: nan, (torch.Size([128, 200]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.bias: nan, (torch.Size([128]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.weight: nan, (torch.Size([512, 512]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.bias: nan, (torch.Size([512]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.weight: nan, (torch.Size([512, 512]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.bias: nan, (torch.Size([512]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.weight: nan, (torch.Size([512, 512]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.bias: nan, (torch.Size([512]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.weight: nan, (torch.Size([1, 1537]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.bias: nan, (torch.Size([1]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: nan, (torch.Size([256, 2376]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-11 22:28:48,213 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: nan, (torch.Size([1536, 2376]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: nan, (torch.Size([1536]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: nan, (torch.Size([1536, 256]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: nan, (torch.Size([1536]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: nan, (torch.Size([1536, 256]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: nan, (torch.Size([1536]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: nan, (torch.Size([256, 2376]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: nan, (torch.Size([1280, 2376]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: nan, (torch.Size([1280]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: nan, (torch.Size([1280, 256]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: nan, (torch.Size([1280]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: nan, (torch.Size([256, 2760]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: nan, (torch.Size([1536, 2760]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: nan, (torch.Size([1536]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: nan, (torch.Size([1536, 256]))
2020-11-11 22:28:48,214 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: nan, (torch.Size([1536]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: nan, (torch.Size([1536, 256]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: nan, (torch.Size([1536]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: nan, (torch.Size([256, 2760]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: nan, (torch.Size([1280, 2760]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: nan, (torch.Size([1280]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: nan, (torch.Size([1280, 256]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: nan, (torch.Size([1280]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.weight      : nan, (torch.Size([1024, 512]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.bias        : nan, (torch.Size([1024]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.weight      : nan, (torch.Size([4096, 1024]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.bias        : nan, (torch.Size([4096]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.weight  : nan, (torch.Size([51, 4096]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.bias    : nan, (torch.Size([51]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.up_dim.weight        : nan, (torch.Size([4096, 2048]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.up_dim.bias          : nan, (torch.Size([4096]))
2020-11-11 22:28:48,215 maskrcnn_benchmark INFO: roi_heads.relation.predictor.freq_bias.obj_baseline.weight: nan, (torch.Size([22801, 51]))
2020-11-11 22:28:48,216 maskrcnn_benchmark INFO: -------------------------------
2020-11-12 10:58:13,030 maskrcnn_benchmark INFO: Using 1 GPUs
2020-11-12 10:58:13,030 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_R_101_FPN_1x.yaml', distributed=False, local_rank=0, opts=['SOLVER.IMS_PER_BATCH', '4', 'TEST.IMS_PER_BATCH', '1', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '90000', 'SOLVER.STEPS', '(45000, 60000)', 'GLOVE_DIR', 'checkpoint/share/glove.6B', 'MODEL.PRETRAINED_DETECTOR_CKPT', 'checkpoint/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', 'data/relation_motif', 'SOLVER.PRE_VAL', 'False'], skip_test=False)
2020-11-12 10:58:13,030 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2020-11-12 10:58:14,479 maskrcnn_benchmark INFO: 
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.5 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: 
GPU 0: GeForce GTX 1650
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 455.32.00
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl  
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py36he904b0f_0  
[conda] mkl_fft                   1.2.0            py36h23d657b_0  
[conda] mkl_random                1.1.1            py36h0573a6f_0  
[conda] pytorch                   1.2.0           py3.6_cuda10.0.130_cudnn7.6.2_0    pytorch
[conda] torchvision               0.4.0                py36_cu100    pytorch
        Pillow (8.0.1)
2020-11-12 10:58:14,480 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_R_101_FPN_1x.yaml
2020-11-12 10:58:14,480 maskrcnn_benchmark INFO: 
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/MSRA/R-101"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 512
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 64
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 2048
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 64
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: True
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #PREDICTOR: "MotifPredictor"
    PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "CausalAnalysisPredictor"
    #PREDICTOR: "TransformerPredictor"
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    # Parameter for Transformer Module
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048 
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
OUTPUT_DIR: './output/relation_baseline'
TEST:
  RELATION:
    REQUIRE_OVERLAP: True
    LATER_NMS_PREDICTION_THRES: 0.5

2020-11-12 10:58:14,482 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GLOVE_DIR: checkpoint/share/glove.6B
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (800,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: checkpoint/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 1
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 64
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 2048
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 64
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: False
    BATCH_SIZE_PER_IMAGE: 64
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: False
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: False
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    LABEL_SMOOTHING_LOSS: True
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: True
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 512
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/MSRA/R-101
OUTPUT_DIR: data/relation_motif
PATHS_CATALOG: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.01
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 4
  MAX_ITER: 90000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (45000, 60000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 1
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: True
    SYNC_GATHER: False
  SAVE_PROPOSALS: False
2020-11-12 10:58:14,483 maskrcnn_benchmark INFO: Saving config into: data/relation_motif/config.yml
2020-11-12 10:58:14,510 maskrcnn_benchmark INFO: #################### prepare training ####################
2020-11-12 10:58:16,627 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-12 10:58:16,627 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2020-11-12 10:58:16,628 maskrcnn_benchmark.data.build INFO: Loading data statistics from: data/relation_motif/VG_stanford_filtered_with_attribute_train_statistics.cache
2020-11-12 10:58:16,628 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-12 10:58:25,316 maskrcnn_benchmark INFO: #################### end model construction ####################
2020-11-12 10:58:25,556 maskrcnn_benchmark INFO: #################### end optimizer and shcedule ####################
2020-11-12 10:58:25,561 maskrcnn_benchmark INFO: #################### end distributed ####################
2020-11-12 10:58:25,561 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from checkpoint/pretrained_faster_rcnn/model_final.pth
2020-11-12 10:58:25,993 maskrcnn_benchmark INFO: #################### end load checkpointer ####################
2020-11-12 10:58:25,993 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2020-11-12 11:04:30,455 maskrcnn_benchmark INFO: Using 1 GPUs
2020-11-12 11:04:30,455 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_R_101_FPN_1x.yaml', distributed=False, local_rank=0, opts=['SOLVER.IMS_PER_BATCH', '4', 'TEST.IMS_PER_BATCH', '1', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '90000', 'SOLVER.STEPS', '(45000, 60000)', 'GLOVE_DIR', 'checkpoint/share/glove.6B', 'MODEL.PRETRAINED_DETECTOR_CKPT', 'checkpoint/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', 'data/relation_motif', 'SOLVER.PRE_VAL', 'False'], skip_test=False)
2020-11-12 11:04:30,455 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2020-11-12 11:04:31,866 maskrcnn_benchmark INFO: 
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.5 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: 
GPU 0: GeForce GTX 1650
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 455.32.00
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl  
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py36he904b0f_0  
[conda] mkl_fft                   1.2.0            py36h23d657b_0  
[conda] mkl_random                1.1.1            py36h0573a6f_0  
[conda] pytorch                   1.2.0           py3.6_cuda10.0.130_cudnn7.6.2_0    pytorch
[conda] torchvision               0.4.0                py36_cu100    pytorch
        Pillow (8.0.1)
2020-11-12 11:04:31,867 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_R_101_FPN_1x.yaml
2020-11-12 11:04:31,867 maskrcnn_benchmark INFO: 
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/MSRA/R-101"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 512
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 64
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 2048
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 64
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: True
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #PREDICTOR: "MotifPredictor"
    PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "CausalAnalysisPredictor"
    #PREDICTOR: "TransformerPredictor"
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    # Parameter for Transformer Module
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048 
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
OUTPUT_DIR: './output/relation_baseline'
TEST:
  RELATION:
    REQUIRE_OVERLAP: True
    LATER_NMS_PREDICTION_THRES: 0.5

2020-11-12 11:04:31,869 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GLOVE_DIR: checkpoint/share/glove.6B
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (800,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: checkpoint/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 1
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 64
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 2048
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 64
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: False
    BATCH_SIZE_PER_IMAGE: 64
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: False
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: False
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    LABEL_SMOOTHING_LOSS: True
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: True
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 512
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/MSRA/R-101
OUTPUT_DIR: data/relation_motif
PATHS_CATALOG: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.01
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 4
  MAX_ITER: 90000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (45000, 60000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 1
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: True
    SYNC_GATHER: False
  SAVE_PROPOSALS: False
2020-11-12 11:04:31,870 maskrcnn_benchmark INFO: Saving config into: data/relation_motif/config.yml
2020-11-12 11:04:31,904 maskrcnn_benchmark INFO: #################### prepare training ####################
2020-11-12 11:04:33,888 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-12 11:04:33,889 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2020-11-12 11:04:33,889 maskrcnn_benchmark.data.build INFO: Loading data statistics from: data/relation_motif/VG_stanford_filtered_with_attribute_train_statistics.cache
2020-11-12 11:04:33,889 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-12 11:04:41,298 maskrcnn_benchmark INFO: #################### end model construction ####################
2020-11-12 11:04:41,551 maskrcnn_benchmark INFO: #################### end optimizer and shcedule ####################
2020-11-12 11:04:41,556 maskrcnn_benchmark INFO: #################### end distributed ####################
2020-11-12 11:04:41,556 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from checkpoint/pretrained_faster_rcnn/model_final.pth
2020-11-12 11:04:41,995 maskrcnn_benchmark INFO: #################### end load checkpointer ####################
2020-11-12 11:04:41,995 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2020-11-12 11:04:43,827 maskrcnn_benchmark.utils.miscellaneous INFO: Saving labels mapping into data/relation_motif/labels.json
2020-11-12 11:04:44,540 maskrcnn_benchmark INFO: #################### end dataloader ####################
2020-11-12 11:04:44,541 maskrcnn_benchmark INFO: Start training
2020-11-12 11:05:26,011 maskrcnn_benchmark INFO: Using 1 GPUs
2020-11-12 11:05:26,011 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_R_101_FPN_1x.yaml', distributed=False, local_rank=0, opts=['SOLVER.IMS_PER_BATCH', '4', 'TEST.IMS_PER_BATCH', '1', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '90000', 'SOLVER.STEPS', '(45000, 60000)', 'GLOVE_DIR', 'checkpoint/share/glove.6B', 'MODEL.PRETRAINED_DETECTOR_CKPT', 'checkpoint/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', 'data/relation_motif', 'SOLVER.PRE_VAL', 'False'], skip_test=False)
2020-11-12 11:05:26,011 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2020-11-12 11:05:27,445 maskrcnn_benchmark INFO: 
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.5 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: 
GPU 0: GeForce GTX 1650
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 455.32.00
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl  
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py36he904b0f_0  
[conda] mkl_fft                   1.2.0            py36h23d657b_0  
[conda] mkl_random                1.1.1            py36h0573a6f_0  
[conda] pytorch                   1.2.0           py3.6_cuda10.0.130_cudnn7.6.2_0    pytorch
[conda] torchvision               0.4.0                py36_cu100    pytorch
        Pillow (8.0.1)
2020-11-12 11:05:27,445 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_R_101_FPN_1x.yaml
2020-11-12 11:05:27,445 maskrcnn_benchmark INFO: 
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/MSRA/R-101"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 512
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 64
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 2048
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 64
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: True
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #PREDICTOR: "MotifPredictor"
    PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "CausalAnalysisPredictor"
    #PREDICTOR: "TransformerPredictor"
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    # Parameter for Transformer Module
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048 
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
OUTPUT_DIR: './output/relation_baseline'
TEST:
  RELATION:
    REQUIRE_OVERLAP: True
    LATER_NMS_PREDICTION_THRES: 0.5

2020-11-12 11:05:27,447 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GLOVE_DIR: checkpoint/share/glove.6B
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (800,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: checkpoint/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 512
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 1
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 64
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 2048
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 64
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: False
    BATCH_SIZE_PER_IMAGE: 64
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: False
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: False
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    LABEL_SMOOTHING_LOSS: True
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: True
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 512
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/MSRA/R-101
OUTPUT_DIR: data/relation_motif
PATHS_CATALOG: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/engineering/Documents/Thesis/Scene-Graph-Benchmark.pytorch/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.01
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 4
  MAX_ITER: 90000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 5000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.3
    MAX_DECAY_STEP: 4
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (45000, 60000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 1
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: True
    SYNC_GATHER: False
  SAVE_PROPOSALS: False
2020-11-12 11:05:27,448 maskrcnn_benchmark INFO: Saving config into: data/relation_motif/config.yml
2020-11-12 11:05:27,480 maskrcnn_benchmark INFO: #################### prepare training ####################
2020-11-12 11:05:29,477 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-12 11:05:29,477 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2020-11-12 11:05:29,478 maskrcnn_benchmark.data.build INFO: Loading data statistics from: data/relation_motif/VG_stanford_filtered_with_attribute_train_statistics.cache
2020-11-12 11:05:29,557 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2020-11-12 11:05:36,949 maskrcnn_benchmark INFO: #################### end model construction ####################
2020-11-12 11:05:37,191 maskrcnn_benchmark INFO: #################### end optimizer and shcedule ####################
2020-11-12 11:05:37,196 maskrcnn_benchmark INFO: #################### end distributed ####################
2020-11-12 11:05:37,196 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from checkpoint/pretrained_faster_rcnn/model_final.pth
2020-11-12 11:05:37,631 maskrcnn_benchmark INFO: #################### end load checkpointer ####################
2020-11-12 11:05:37,632 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2020-11-12 11:05:39,479 maskrcnn_benchmark.utils.miscellaneous INFO: Saving labels mapping into data/relation_motif/labels.json
2020-11-12 11:05:40,208 maskrcnn_benchmark INFO: #################### end dataloader ####################
2020-11-12 11:05:40,208 maskrcnn_benchmark INFO: Start training
2020-11-12 11:05:41,964 maskrcnn_benchmark INFO: ---Total norm nan clip coef nan-----------------
2020-11-12 11:05:41,970 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: nan, (torch.Size([512, 2048, 3, 3]))
2020-11-12 11:05:41,970 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: nan, (torch.Size([512]))
2020-11-12 11:05:41,970 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: nan, (torch.Size([2048, 25088]))
2020-11-12 11:05:41,970 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: nan, (torch.Size([2048]))
2020-11-12 11:05:41,970 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: nan, (torch.Size([2048, 2048]))
2020-11-12 11:05:41,970 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: nan, (torch.Size([2048]))
2020-11-12 11:05:41,970 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.weight: nan, (torch.Size([256, 2, 7, 7]))
2020-11-12 11:05:41,970 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.bias: nan, (torch.Size([256]))
2020-11-12 11:05:41,970 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.weight: nan, (torch.Size([256]))
2020-11-12 11:05:41,970 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.bias: inf, (torch.Size([256]))
2020-11-12 11:05:41,970 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.weight: nan, (torch.Size([512, 256, 3, 3]))
2020-11-12 11:05:41,970 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.bias: nan, (torch.Size([512]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.weight: nan, (torch.Size([512]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.bias: nan, (torch.Size([512]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([2048, 25088]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([2048]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([2048, 2048]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([2048]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed1.weight: nan, (torch.Size([151, 200]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed2.weight: nan, (torch.Size([151, 200]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.weight: nan, (torch.Size([32, 9]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.bias: nan, (torch.Size([32]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.weight: nan, (torch.Size([32]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.bias: nan, (torch.Size([32]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.weight: nan, (torch.Size([128, 32]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.bias: nan, (torch.Size([128]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.weight: nan, (torch.Size([128, 6]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.bias: nan, (torch.Size([128]))
2020-11-12 11:05:41,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.weight: nan, (torch.Size([128]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.bias: nan, (torch.Size([128]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.weight: nan, (torch.Size([128, 9]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.bias: nan, (torch.Size([128]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.weight: nan, (torch.Size([128]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.bias: nan, (torch.Size([128]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bi_freq_prior.weight: nan, (torch.Size([1, 22801]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.weight: nan, (torch.Size([128, 2048]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.bias: nan, (torch.Size([128]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.weight: nan, (torch.Size([128, 200]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.bias: nan, (torch.Size([128]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.weight: nan, (torch.Size([512, 512]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.bias: nan, (torch.Size([512]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.weight: nan, (torch.Size([512, 512]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.bias: nan, (torch.Size([512]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.weight: nan, (torch.Size([512, 512]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.bias: nan, (torch.Size([512]))
2020-11-12 11:05:41,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.weight: nan, (torch.Size([1, 1537]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.bias: nan, (torch.Size([1]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: nan, (torch.Size([256, 2376]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: nan, (torch.Size([1536, 2376]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: nan, (torch.Size([1536]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: nan, (torch.Size([1536, 256]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: nan, (torch.Size([1536]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: nan, (torch.Size([1536, 256]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: nan, (torch.Size([1536]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: nan, (torch.Size([256, 2376]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: nan, (torch.Size([1280, 2376]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: nan, (torch.Size([1280]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: nan, (torch.Size([1280, 256]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: nan, (torch.Size([1280]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: nan, (torch.Size([256, 2760]))
2020-11-12 11:05:41,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: nan, (torch.Size([1536, 2760]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: nan, (torch.Size([1536]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: nan, (torch.Size([1536, 256]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: nan, (torch.Size([1536]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: nan, (torch.Size([1536, 256]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: nan, (torch.Size([1536]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: nan, (torch.Size([256, 2760]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: nan, (torch.Size([256]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: nan, (torch.Size([1280, 2760]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: nan, (torch.Size([1280]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: nan, (torch.Size([1280, 256]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: nan, (torch.Size([1280]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.weight      : nan, (torch.Size([1024, 512]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.bias        : nan, (torch.Size([1024]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.weight      : nan, (torch.Size([4096, 1024]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.bias        : nan, (torch.Size([4096]))
2020-11-12 11:05:41,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.weight  : nan, (torch.Size([51, 4096]))
2020-11-12 11:05:41,975 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.bias    : nan, (torch.Size([51]))
2020-11-12 11:05:41,975 maskrcnn_benchmark INFO: roi_heads.relation.predictor.up_dim.weight        : nan, (torch.Size([4096, 2048]))
2020-11-12 11:05:41,975 maskrcnn_benchmark INFO: roi_heads.relation.predictor.up_dim.bias          : nan, (torch.Size([4096]))
2020-11-12 11:05:41,975 maskrcnn_benchmark INFO: roi_heads.relation.predictor.freq_bias.obj_baseline.weight: nan, (torch.Size([22801, 51]))
2020-11-12 11:05:41,975 maskrcnn_benchmark INFO: -------------------------------
